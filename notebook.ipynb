{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# IMPORT MODULES\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import platform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt \n",
    "import sklearn\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns \n",
    "import hyperopt\n",
    "import scipy\n",
    "import xgboost\n",
    "\n",
    "# Print Anaconda and Python versions\n",
    "print(f\"Anaconda Version is {sys.version}\")\n",
    "print(f\"Python version is {platform.python_version()}\")\n",
    "\n",
    "# Print versions of imported libraries\n",
    "print(f\"Pandas version is: {pd.__version__}\")\n",
    "print(f\"Numpy version is: {np.__version__}\")\n",
    "print(f\"Matplotlib version is: {matplotlib.__version__}\")\n",
    "print(f\"Sklearn version is: {sklearn.__version__}\")\n",
    "print(f\"Seaborn version is: {sns.__version__}\")\n",
    "print(f\"Hyperopt version is: {hyperopt.__version__}\")\n",
    "print(f\"Scipy version is: {scipy.__version__}\")\n",
    "print(f\"Xgboost version is: {xgboost.__version__}\")\n",
    "\n",
    "# Set seaborn settings\n",
    "sns.set(context='paper', palette='winter_r', style='darkgrid', rc={'figure.facecolor': 'gray'}, font_scale=1.5)\n",
    "\n",
    "# Filter out warnings (deprecations, etc.)\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Alter ast_note_interactivity kernel option so jupyter displays multiple statements at once\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all modules/libraries which will be loaded within this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV,  RandomizedSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_curve, auc, classification_report, confusion_matrix\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "##Importing hyperopt libraries\n",
    "from functools import partial\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval\n",
    "from hyperopt.pyll import scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Data Analysis\n",
    "---\n",
    "\n",
    "##### 2.1 Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2.1 Data Exploration\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Construct the file path\n",
    "file_path = os.path.join(current_directory, 'data', 'oasis_longitudinal_demographics.csv')\n",
    "\n",
    "# Load in the data\n",
    "mri = pd.read_csv(file_path)\n",
    "\n",
    "# Get a feel for the data - observe first 5 rows\n",
    "print(\"First 5 rows of the data:\")\n",
    "print(mri.head(5))\n",
    "\n",
    "# Print out the number of columns in the dataframe (features)\n",
    "num_columns = len(mri.columns)\n",
    "print(f\"\\nThere are {num_columns} attributes within the dataset.\")\n",
    "\n",
    "# Check the columns of the dataset in prep for Data Exploration\n",
    "print(\"\\nColumns of the dataset:\")\n",
    "print(mri.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the columns and their type - info prints this info about a dataframe\n",
    "print(\"Information about the dataset:\")\n",
    "mri.info()\n",
    "\n",
    "# Dimensions of the dataset\n",
    "num_columns = len(mri.columns)\n",
    "num_rows = len(mri.index)\n",
    "print(f\"\\nThe dataset is made up with {num_columns} columns and {num_rows} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's check the distinct values in certain columns\n",
    "# Common sense would suggest that Subject ID, MRI ID, MR Delay, eTIV, nWBV, and ASF features would have too many distinct values for the purpose of this test.\n",
    "# Therefore let's ignore them\n",
    "\n",
    "num_columns = ['Visit', 'Age', 'EDUC', 'SES', 'MMSE', 'CDR']\n",
    "\n",
    "for col in num_columns:\n",
    "    unique_values = mri[col].unique()\n",
    "    print(f\"{len(unique_values)} unique values for {col}:\")\n",
    "    print(unique_values)\n",
    "    print(\"\\nValue counts:\")\n",
    "    print(mri[col].value_counts(), \"\\n\")\n",
    "\n",
    "cat_columns = ['Group', 'M/F', 'Hand']\n",
    "for col in cat_columns:\n",
    "    unique_values = mri[col].unique()\n",
    "    print(f\"{len(unique_values)} unique values for {col}:\")\n",
    "    print(unique_values)\n",
    "    print(\"\\nValue counts:\")\n",
    "    print(mri[col].value_counts(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get a summary of the numerical attributes within our dataset - let's us see if we need to normalize/scale at a later point\n",
    "for col in num_columns:\n",
    "    mri[col].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting histograms side by side for pairs of variables\n",
    "\n",
    "# Define pairs of variables\n",
    "pairs = [('Age', 'CDR'), ('SES', 'EDUC'), ('MMSE', 'M/F')]\n",
    "\n",
    "# Plot histograms for each pair of variables\n",
    "for pair in pairs:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    for i, col in enumerate(pair):\n",
    "        sns.histplot(mri[col], ax=axes[i], kde=False)\n",
    "        axes[i].set_title(f'Histogram of {col}')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "# Adding ; to last statement to get rid of <matplotlib...> <seaborn...> text\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things can be concluded from these graphs about the dataset:\n",
    "- It seems that the age of the majority of Subject IDs falls between 70 - 85 with the most frequent age being 73 years old (more than 25 participants in this study are of this age). Note this could be skewed as we have not yet discounted Subject IDs that have had more than one session. \n",
    "- More than half of the datasets' population received a CDR rating of 0 (>200 Subject IDs) while less than 10 subjects were diagnosed with a 2.0 CDR score.  \n",
    "- The most frequent Socio-Economic-status score in this study was 2.0 with over > 100 subject IDs (103), followed by > 85 subject IDs graded as 1.0 on the SES scale. \n",
    "- Most of the individuals in this study undertook a significant amount of years in Education with over 2/3s of the study having at least studied 12+ years of Education. \n",
    "- The majority of Subject IDs scored highly in their MMSE examinations (> 50% scoring 29 or above).\n",
    "- The percentage ratio between females and males was 57:43. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**:\n",
    "\n",
    "Although i will be replacing the 'Converted' values in the group attribute, for the purpose of visual analysis, I am dropping the rows for 'Converted' patients so i can directly compare 'demented' and 'non-demented' subjects. To accomodate this, I am taking a copy of the 'mri' dataframe to ensure we dont lose values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function called `plot_facetgrid` which will generate FacetGrid plots based on specified variables, using predefined colors, titles, and x-axis limits. It utilizes Seaborn's FacetGrid and kdeplot functions for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe so we can remove the 'converted' value from the 'Group' attribute\n",
    "mri_copy = mri.copy()\n",
    "\n",
    "mri_copy =mri_copy[mri_copy.Group != \"Converted\"]\n",
    "\n",
    "def plot_facetgrid(data, var):\n",
    "    \"\"\"\n",
    "    Function to create a FacetGrid chart based on the variable var.\n",
    "    \n",
    "    Parameters:\n",
    "        data (DataFrame): The DataFrame containing the data.\n",
    "        var (str): The variable to plot.\n",
    "    \"\"\"\n",
    "    csfont_options = { 'fontname': 'monospace',\n",
    "                   'weight': 'bold',\n",
    "                   'color':'b',\n",
    "                   'size': 'large'\n",
    "                  }\n",
    "    \n",
    "    plot_colors_lookup = {\n",
    "        'MMSE': {'color': ['r', 'k'], 'title': 'Dementia Variation against MMSE scores', 'xlim': (10, data['MMSE'].max())},\n",
    "        'SES': {'color': ['r', 'k'], 'title': 'Variation of Dementia against Socio-Economic-Status', 'xlim': (0, data['SES'].max())},\n",
    "        'nWBV': {'color': ['r', 'k'], 'title': 'Variation of Dementia against Normalized Whole Brain Volume', 'xlim': (0, data['nWBV'].max())},\n",
    "        'eTIV': {'color': ['r', 'k'], 'title': 'Variation of Dementia against Estimated Total Intracranial volume', 'xlim': (0, data['eTIV'].max())},\n",
    "        'EDUC': {'color': ['r', 'k'], 'title': 'Variation of Dementia against Years of Education', 'xlim': (3, data['EDUC'].max())},\n",
    "        'Age': {'color': ['r', 'k'], 'title': 'Dementia Variation against Age', 'xlim': (10, data['Age'].max())}\n",
    "    }\n",
    "    \n",
    "    # Lookup plot colors, title, and xlim based on the variable var\n",
    "    plot_colors = plot_colors_lookup[var]['color']\n",
    "    title = plot_colors_lookup[var]['title']\n",
    "    xlim = plot_colors_lookup[var]['xlim']\n",
    "    \n",
    "    # Create FacetGrid\n",
    "    g = sns.FacetGrid(data, hue='Group', hue_kws={'color': plot_colors}, height=8)\n",
    "    \n",
    "    # Plot KDE plot\n",
    "    g.map(sns.kdeplot, var, shade=True)\n",
    "    \n",
    "    # Set x-axis limit\n",
    "    g.set(xlim=xlim)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    # Set title\n",
    "    plt.title(title, **csfont_options)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_facetgrid(mri_copy, 'MMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, one can see the Dementia distribution as a function of MMSE score. For high MMSE scores, in approximately 70% of cases, the subject IDs are generally considered as non-demented . Finally, as expected, for low MMSE scores (<22) all subjects receive a 'demented' diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_facetgrid(mri_copy, 'SES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the distribution of Dementia across the study is greatest for subjects who have receive a score in the socio-economic-status scale of 3 or more. This possibly suggests that the more afluent an individual is, the greater the chance of Dementia. Subjects who generally score 3 or less are considered 'Non-Demented' (in >50% of cases for a score of 2) or 'Converted' (in ~60% of cases where subjects score 1 in the SES scale)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_facetgrid(mri_copy, 'nWBV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one would expect, subjects who exhibited a greater percentage of normalized whole brain volume, had a greater chance of receiving a 'Non-demented' diagnosis.  The lower the nWBV value, the greater the chance a patient was classified as 'demented' or 'converted'. In recent years, brain shrinkage has become a potential important marker for early changes in the brain tissue where such symptoms have been associated as early markers for ailments of Alzheimer's [https://www.webmd.com/alzheimers/news/20110413/brain-shrinkage-may-help-predict-alzheimers]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_facetgrid(mri_copy, 'eTIV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, there didn't seem to be much difference between the variation of Dementia with ETIV i.e. the estimated total intracranial volume didn't vary much over time and therefore ETIV was not significantly different between any of classification groups. As referenced in the Paper \"Inracranial Volume and Alzheimer Disease: Evidence agaisnt the cerebal Reserve Hypothesis\" by Jenkins et al,  the Mean TIV did not differ significantly between subjects and that the only significant predictor of TIV was sex. Thus it was concluded that their findings do not suggest that a larger brain provides better 'protection' against AD [https://www.ncbi.nlm.nih.gov/pubmed/10681081]. This explains the visual cues seen in the plot above  .i.e 'Demented' and 'nondemented' subjects have relatively the same ETIV values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_facetgrid(mri_copy, 'EDUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems from quickly glancing at the above, Dementia is more prevelant in subjects who had fewer years in Education. For e.g. any subject who had less than 13 years of Education had a slightly increased possibility of developing Dementia where as the chances of having Dementia-like symptoms (on first visit) decreases quite sharply for subjects who have had 17+ years in Education."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, Dementia is more prevelant in subjects who fall within the 65 - 85 year group. Before 65, one could hypothesise that the subject is too young for the full blown effects of Dementia to manifest. Similarly, after 85, given that a patient could have been living with the disease for many years,certain symptoms could have had a profound effect on an individual's life. This possibly explains why the dementia (not converted) diagnosis is small after the years of 85. Obviously, Dementia forms in people at different stages, as shown by the converted plot where it gets larger in magnitude as ages increase (up to ~85 years old). This would suggest that a subject's condition has been updated from a 'non-demented' to 'demented' status in that timeframe. For the purpose of visual analysis and for reasons of brevity, I'm only discussing the direct comparisons between the 'demented' and 'non-demented' plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_facetgrid(mri_copy, 'Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see the 'Converted' group plotted against both 'demented' & 'non-demented' groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the target variable and features\n",
    "targ = 'Group'\n",
    "cols = ['MMSE', 'SES', 'nWBV', 'eTIV', 'EDUC', 'Age']\n",
    "\n",
    "# Define font options for the title\n",
    "csfont_options = {'fontname': 'monospace', 'weight': 'bold', 'color': 'b', 'size': 'large'}\n",
    "\n",
    "# Set min x-axis values for each feature\n",
    "xlimits = [12, 0, 0.55, 840, 3, 50]\n",
    "\n",
    "# Define plot colors for different groups\n",
    "plot_colors = {'Converted': 'r', 'Demented': 'k', 'Non-Demented': 'b'}\n",
    "\n",
    "# Iterate over the target variable and features\n",
    "for feature, xlimit in zip(cols, xlimits):\n",
    "    g = sns.FacetGrid(mri, hue=targ, height=8)\n",
    "    g.map(sns.kdeplot, feature, shade=True)\n",
    "    g.set(xlim=(10, mri[feature].max()))\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Dementia Variation against {}'.format(feature), **csfont_options)\n",
    "    plt.xlim(xlimit)\n",
    "\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiousity, plot the frequency gender for Demented/Non-Demented subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.catplot(x='M/F', \n",
    "                data=mri_copy, hue='Group', \n",
    "                kind='count', palette=\"muted\", \n",
    "                legend=False, height=6)\n",
    "\n",
    "g.despine(left=True)\n",
    "g.set_ylabels(\"Participation count\")\n",
    "plt.legend(loc='best')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_corr_heatmap(data, method, heading):\n",
    "    \"\"\"\n",
    "    Plot correlation heatmap for the given data.\n",
    "\n",
    "    Parameters:\n",
    "        data (DataFrame): The DataFrame containing the data.\n",
    "        method (str): The method used for computing correlations.\n",
    "        heading (str): The title for the heatmap.\n",
    "    \"\"\"\n",
    "    # Copy mri dataset and preprocess\n",
    "    mri_copy = data.drop(['Subject ID', 'M/F', 'MRI ID', 'Hand'], axis=1)\n",
    "    mri_copy = pd.get_dummies(mri_copy, drop_first=True)\n",
    "\n",
    "    # Compute correlation matrix\n",
    "    corr = mri_copy.corr(method=method)\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Draw the heatmap\n",
    "    sns.heatmap(corr, annot=True, linewidths=.5, fmt='.2f', ax=ax)\n",
    "\n",
    "    # Set title\n",
    "    ax.set_title(heading)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    return corr\n",
    "\n",
    "plot_corr_heatmap(mri, 'pearson', 'Correlation Heat map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the correlation coefficient (pearsons r) between each pair of attributes, I can make several assumptions:\n",
    "- CDR has a strong positive correlation with both Group_Demented and Group_Nondemented. This is expected given that the CDR scale is used to gauge whether a patient has symptoms of Dementia. Given that this feature is strongly correlated with the target attribute, it would be advisable to drop this column during feature selection.\n",
    "- Age and nWBV have quite a strong negative correlation. \n",
    "- EDUC has quite strong correlations with MMSE, eTIV and a v.strong negative correlation with SES\n",
    "- MMSE and CDR share a strong -ive correlation , quite strong correlation with nWBV. \n",
    "- Most of the attributes share a correlation that is close to 0 thus we assume that there is little to no linear correlation\n",
    "\n",
    "Note, these **correlations are only linear and as a result we arent measuring non-linear relationships that could exist within the dataset (if attribute x is close to 0, y goes up) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3 Scatter Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scatter_matrix(mri[mri.columns], figsize=(12,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Data preprocessing\n",
    "---\n",
    "\n",
    "##### 3.1 Categorical Attribute replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display unique values before and after replacement\n",
    "for col in ['Group', 'M/F']:\n",
    "    print(f\"Before replacement {col} has the following values: {mri[col].unique()}\")\n",
    "\n",
    "# Create a mapping dictionary for replacement\n",
    "mapping = {'F': 0, 'M': 1, 'Nondemented': 0, 'Demented': 1, 'Converted': 1}\n",
    "\n",
    "# Replace categorical values with numerical values\n",
    "mri.replace({'Group': mapping, 'M/F': mapping}, inplace=True)\n",
    "\n",
    "# Display unique values after replacement\n",
    "for col in ['Group', 'M/F']:\n",
    "    print(f\"After replacement {col} has the following numerical values: {mri[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Using LabelEncoder from scikit learn to encode categorical features to numerical.\n",
    "obj_mri = mri.select_dtypes(include=['object'])\n",
    "\n",
    "# Converting categorical Data \n",
    "for col in obj_mri.columns:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(mri[col])\n",
    "    list(le.classes_)\n",
    "    mri[col] = le.transform(mri[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that all columns are now numerical\n",
    "mri.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2 Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outlier_detection(cols):\n",
    "    \"\"\"\n",
    "    Detect outliers in a list of values using z-score method.\n",
    "    \n",
    "    Parameters:\n",
    "        cols (list): A list of numerical values.\n",
    "    \n",
    "    Returns:\n",
    "        outliers (tuple): A tuple containing the indices of outliers.\n",
    "    \"\"\"\n",
    "    mean_col = np.mean(cols)\n",
    "    std_col = np.std(cols)\n",
    "    threshold = 3\n",
    "    z_scores = [(col - mean_col) / std_col for col in cols]\n",
    "    outliers = np.where(np.abs(z_scores) > threshold)\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = [\"EDUC\", \"SES\", \"MMSE\", \"CDR\", \"eTIV\", \"Visit\", \"eTIV\", \"nWBV\", \"ASF\"]\n",
    "\n",
    "for col in cols:\n",
    "    outliers_indices = outlier_detection(mri[col])\n",
    "    if len(outliers_indices[0]) > 0:\n",
    "        print(f\"Outliers for {col} are given below:\")\n",
    "        for i in outliers_indices[0]:\n",
    "            outlier_value = mri[col].iloc[i]\n",
    "            print(f\"Index: {i}, Value: {outlier_value}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outliers for MMSE, CDR and Visit have actual 'meaning' ... i.e. MMSE scores of 16,15,7,4 etc are used to determine the severity of Dementia in a subject (same applies to CDR). Thus, I believe it wouldn't be suitable to drop these outliers from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.3 Imputation - dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check which columns contain null values\n",
    "nulls = mri.columns[mri.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check how many nulls in each column\n",
    "mri[nulls].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation - only works on numerical attributes, hence why we encoded the categorical text features in 3.1\n",
    "# Impute missing values with the median value of the attribute\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Final check to make sure all columns are numerical before proceeding with imputation\n",
    "for column in mri.columns:\n",
    "    if mri[column].dtype == 'object':\n",
    "        print(f\"{column} of type object found!\")\n",
    "    else:\n",
    "        print(f\"{column} is a numerical attribute - proceed with imputation\")\n",
    "\n",
    "# Fitting imputer instance\n",
    "imputer.fit(mri)\n",
    "\n",
    "# Check if the median computed by the imputer matches the median of the original data\n",
    "print(\"Medians match:\", (imputer.statistics_ == mri.median().values).all())\n",
    "\n",
    "# Transforming mri data to replace null values via imputation\n",
    "X = imputer.transform(mri)\n",
    "\n",
    "# Constructing dataframe from transformed values\n",
    "mri = pd.DataFrame(X, columns=mri.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check that nulls have been replaced\n",
    "mri[nulls].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.4 Standardizing dataframe attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = ['Age', 'M/F', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF']\n",
    "\n",
    "max_values = mri[cols].max()\n",
    "min_values = mri[cols].min()\n",
    "\n",
    "print(f\"max value in mri dataset for {cols} before scaling:\\n{max_values}\")\n",
    "print(f\"min value in mri dataset for {cols} before scaling:\\n{min_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(mri)\n",
    "mri[cols]=scaler.fit_transform(mri[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = ['Age', 'M/F', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF']\n",
    "\n",
    "max_values = mri[cols].max()\n",
    "min_values = mri[cols].min()\n",
    "\n",
    "print(f\"max value in mri dataset for {cols} before scaling:\\n{max_values}\")\n",
    "print(f\"min value in mri dataset for {cols} before scaling:\\n{min_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: We only standardized certain attributes, as some of these columns will be dropped during feature selection in our next preprocessing step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.5 Dropping features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop unneccessary features and also the highly correlated feature CDR from the dataframe\n",
    "mri = mri.drop(['Subject ID', 'Visit', 'MRI ID', 'Hand', 'MR Delay', 'CDR'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mri.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.6 Splitting data into training / test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = np.random.seed(42)\n",
    "y =  mri['Group']   #Target \n",
    "X = mri.drop(['Group'], axis=1) # Features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "#Create copies of our train/test sets for benchmark testing\n",
    "BM_X_train = X_train.copy()\n",
    "BM_X_test = X_test.copy()\n",
    "BM_y_train = y_train.copy()\n",
    "BM_y_test = y_test.copy()\n",
    "\n",
    "#Create further copies incase of further optimization techniques\n",
    "Copy_X_train = X_train.copy()\n",
    "Copy_X_test = X_test.copy()\n",
    "Copy_y_train = y_train.copy()\n",
    "Copy_y_test = y_test.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"X_train set is made up of {} rows and {} columns\\n\".format(len(X_train.index), len(X_train.columns)))\n",
    "print(\"X_test set is made up of {} rows and {} columns\\n\".format(len(X_test.index), len(X_test.columns)))\n",
    "print(\"y_train is made up of {} values\\n\".format(len(y_train.index)))\n",
    "print(\"y_test is made up of {} values\\n\".format(len(y_test.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Data Implementation\n",
    "\n",
    "---\n",
    " **Benchmark Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## As per the evaluators suggestion, look into using xgboost  -> popular boosting model -> appropiate here\n",
    "#setting random state as 42 for reproducability\n",
    "\n",
    "# Define the models\n",
    "models = [ \n",
    "    LogisticRegression(random_state=42),\n",
    "    SVC(random_state=42),\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    XGBClassifier(random_state=42),\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    GaussianNB()\n",
    "]\n",
    "\n",
    "# Define the corresponding classifier names\n",
    "classifiers = [\n",
    "    'Logistic Regression',\n",
    "    'SVC',\n",
    "    'Decision Tree Classifier',\n",
    "    'AdaBoost Classifier',\n",
    "    'Random Forest Classifier',\n",
    "    'XGBClassifier',\n",
    "    'GradientBoostingClassifier',\n",
    "    'Naive Bayes'\n",
    "]\n",
    "\n",
    "# Define the columns for the benchmark dataframe\n",
    "ml_columns = ['ML Algo name', 'Model Parameters', 'training AUC accuracy', 'test recall score', 'test AUC score']\n",
    "\n",
    "# Create an empty dataframe to store benchmark results\n",
    "benchmark = pd.DataFrame(columns=ml_columns)\n",
    "\n",
    "# Perform cross-validation and evaluation for each model\n",
    "for model, clf_name in zip(models, classifiers):\n",
    "    benchmark.loc[len(benchmark)] = [clf_name, str(model.get_params()), np.nan, np.nan, np.nan]\n",
    "    cv_score = cross_val_score(model, BM_X_train, BM_y_train, cv=5, scoring='roc_auc') \n",
    "    benchmark.loc[benchmark['ML Algo name'] == clf_name, 'training AUC accuracy'] = np.mean(cv_score)\n",
    "    mdl = model.fit(BM_X_train, BM_y_train)\n",
    "    test_acc = mdl.score(BM_X_test, BM_y_test)\n",
    "    fpr, tpr, thresholds = roc_curve(BM_y_test, model.predict(BM_X_test))\n",
    "    test_recall = recall_score(BM_y_test, model.predict(BM_X_test))\n",
    "    test_auc = auc(fpr, tpr)\n",
    "    benchmark.loc[benchmark['ML Algo name'] == clf_name, 'test recall score'] = test_recall\n",
    "    benchmark.loc[benchmark['ML Algo name'] == clf_name, 'test AUC score'] = test_auc \n",
    "\n",
    "# Sort benchmark table by test AUC score\n",
    "benchmark.sort_values(by='test AUC score', ascending=False, inplace=True)\n",
    "\n",
    "# Plot the benchmark results\n",
    "sns.barplot(x='test AUC score', y='ML Algo name', data=benchmark)\n",
    "\n",
    "# Set the width of the model parameters column\n",
    "benchmark.style.set_properties(subset=['Model Parameters'], **{'width': '300px'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the roc_curves and confusion metrics for evaluation purposes ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_fpr = {}\n",
    "total_tpr = {}\n",
    "\n",
    "def roc_curves(model, classifier):\n",
    "    ML_name = model.__class__.__name__\n",
    "    fpr, tpr, thresholds = roc_curve(BM_y_test, model.predict(BM_X_test))\n",
    "    test_auc = auc(fpr, tpr)\n",
    "    total_fpr[ML_name] = fpr\n",
    "    total_tpr[ML_name] = tpr\n",
    "    plt.plot(fpr, tpr, color='darkorange', linewidth=2, label='(Area = %0.2f)' % test_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.title('ROC Curve for {}'.format(classifier))\n",
    "    plt.show()\n",
    "\n",
    "def confusion_matrix_plot(model, classifier):\n",
    "    cm = confusion_matrix(y_test, model.predict(BM_X_test))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.get_cmap('Blues'))\n",
    "    labels = ['Nondemented', 'Demented']\n",
    "    plt.title('Confusion Matrix for {}'.format(classifier))\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    s = [['TN', 'FP'], ['FN', 'TP']]\n",
    "    for i, j in itertools.product(range(2), range(2)):\n",
    "        plt.text(j, i, str(s[i][j]) + \" = \" + str(cm[i][j]))\n",
    "    plt.show()\n",
    "\n",
    "for model, classifier in zip(models, classifiers):\n",
    "    roc_curves(model, classifier)\n",
    "    confusion_matrix_plot(model, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting all of the ROC curves on one graph ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = {\n",
    "    'LogisticRegression': 'red',\n",
    "    'SVC': 'green',\n",
    "    'DecisionTreeClassifier': 'blue',\n",
    "    'AdaBoostClassifier': 'yellow',\n",
    "    'RandomForestClassifier': 'cyan',\n",
    "    'XGBClassifier': 'magenta',\n",
    "    'GradientBoostingClassifier': 'black',\n",
    "    'GaussianNB': 'white'\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "for model_name in total_fpr.keys():\n",
    "    plt.plot(total_fpr[model_name], total_tpr[model_name], color=colors[model_name], lw=1, label=model_name)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.title('Comparison of ROC curves for all classifiers')\n",
    "plt.plot([0, 1], [0, 1], color='darkorange', lw=2, linestyle='--')\n",
    "plt.legend()\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2 Implementation\n",
    "---\n",
    "**Optimization techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made the mistake of trying to perform GridSearchCV for all classifiers. As you can see in the markdown below, the hyperparameter tuning of the Random Forests Classifier and the decision tree classifier took approximately 135746 & 37069 seconds or in more explicit scaling, upwards  of 38 / 10 hours. \n",
    "\n",
    "Thus, I will **only be performing GridSearchCV** for LogisticRegression and SVM.\n",
    "\n",
    "The **remaining classifiers will be tuned using RandomizedSearchCV and hyperopt**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch/RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function 'grid' uses GridSearchCV to find optimal hyperparamters for the Logistic Regression and SVM Models. Due to the vast parameter grids for the remaining Models, it was advisable to tune their hyperparameters via RandomizedSearchCV. \n",
    "\n",
    "Also, the 3 benchmark models (SVM, RandomForestClassifier,XGBoost) which i wanted to pay particular attention to,are also fitted for the evaluation metric 'accuracy'. Reasoning behind this - I wanted to collect the classification error on training/test sets ... the classification error being equal to (1 - accuracy_score)... to test the robustness of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Create empty final results dataframe for optimization metrics\n",
    "refinement_cols = ['ML Model','Method','fitting time', 'optimal hyperparameters', 'best training AUC score', 'best estimator','test recall score', 'test AUC score']\n",
    "finale = pd.DataFrame(columns = refinement_cols)\n",
    "\n",
    "#Create empty robustness table to compare training and testing errors\n",
    "robust_cols = ['ML Model', 'Training Classification Error', 'Test Classification_Error']\n",
    "robustment_test = pd.DataFrame(columns = robust_cols)\n",
    "\n",
    "#Storing training times\n",
    "training_time = []\n",
    "#Create two empty dictionaries so that the FPR and TPR can be captured for each model. This will allow us to plot all the ROC curves on one graph\n",
    "total_fpr_opt = {} \n",
    "total_tpr_opt = {}\n",
    "\n",
    "\n",
    "\n",
    "#Define classifiers for tuning - set random_state for reproducible results\n",
    "classifiers = [ LogisticRegression(random_state=42),\n",
    "                SVC(random_state=42),\n",
    "                AdaBoostClassifier(random_state=42),\n",
    "                DecisionTreeClassifier(random_state=42),\n",
    "                RandomForestClassifier(random_state=42),\n",
    "                GradientBoostingClassifier(random_state=42),\n",
    "                XGBClassifier(random_state=42) ]\n",
    "\n",
    "\n",
    "\n",
    "lr_parameters = { \n",
    "                  'C':[0.001, 0.01, 0.1, 1, 10,100,1000, 10000], \n",
    "                  'max_iter':list(range(2,100))}\n",
    "\n",
    "\n",
    "svm_parameters = {\n",
    "                  'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "                  'C':[0.00001,0.0001, 0.001, 0.01, 0.1, 1, 10, 20, 30, 40, 50 ,60 , 80, 100],\n",
    "                  'gamma': [0.00001,0.0001, 0.001, 0.01, 0.1, 1, 10] }\n",
    "\n",
    "dt_parameters = { #'max_leaf_nodes': list(range(2,100)),\n",
    "                  'max_leaf_nodes': list(range(2,20)),\n",
    "                  'splitter':['random', 'best'],\n",
    "                  'criterion': ['gini', 'entropy'],\n",
    "                  'max_depth': list(range(2,20)),\n",
    "                  'min_samples_split': list(range(2,20))}\n",
    "\n",
    "rf_parameters = { \n",
    "                  'n_estimators': [10,30,50,100,150,200,250,300,350,400,500,600,700,800,900,1000],\n",
    "                  'max_leaf_nodes': list(range(2,30)),\n",
    "                  'min_samples_leaf':[1,2,3], 'min_samples_split':[3,4,5,6,7]}\n",
    "\n",
    "gb_parameters = {\n",
    "                  'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                  'min_samples_leaf': list(range(1,20)),\n",
    "                  'max_depth': list(range(2,20)),\n",
    "                  'n_estimators': range(1,200)}\n",
    "\n",
    "xgboost_parameters = { \n",
    "                  'silent': [True],\n",
    "                  'max_depth': [6, 10, 15, 20],\n",
    "                  'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7],\n",
    "                  'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                  'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                  'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                  'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n",
    "                  'gamma': [0, 0.25, 0.5, 1.0],\n",
    "                  'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n",
    "                  'n_estimators': [50,100,120,150]}\n",
    "\n",
    "\n",
    "ada_parameters = { \n",
    "                   'n_estimators': [50,100,200,250,300,400,500,600,700,800,900,1000],\n",
    "                   'learning_rate': [ 0.1,0.15, 0.2,0.25, 0.3,0.4,0.5,0.6,0.7,0.8,0.9,1] }\n",
    "\n",
    "parameters = [lr_parameters, \n",
    "              svm_parameters,\n",
    "              ada_parameters, \n",
    "              dt_parameters,\n",
    "              rf_parameters, \n",
    "              gb_parameters, \n",
    "              xgboost_parameters]  \n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PARALLEL_JOBS\"] = \"16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_jobs():\n",
    "    parallel_jobs = os.getenv('PARALLEL_JOBS')\n",
    "    if parallel_jobs is not None:\n",
    "        try:\n",
    "            n_jobs = int(parallel_jobs)\n",
    "            return n_jobs\n",
    "        except ValueError:\n",
    "            print(\"Error: PARALLEL_JOBS is not an integer. Using default value (None).\")\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "parallel_jobs = get_n_jobs()\n",
    "\n",
    "def print_summary(ML_name, mdl, method, fitting_time, best_params, confusion_matrix, complete_time,parallel_jobs,feature_importances=False):\n",
    "    print(\"--\" * 50)\n",
    "    print(\"Finding the best hyperparameters for metric [roc_auc] for {}\\n\".format(ML_name))\n",
    "    print(\"Performing {} for {}\\n\".format(method, ML_name))\n",
    "    print(\"Beginning to fit {}\\n\".format(ML_name))\n",
    "    print(\"Optimal hyperparameters:\", best_params)\n",
    "    print(\"Confusion matrix:\")\n",
    "    confusion_matrix_plot(confusion_matrix, ML_name)\n",
    "    if feature_importances:\n",
    "        print_feature_importances(ML_name, mdl)\n",
    "    if parallel_jobs == None:\n",
    "        parallel_jobs = 1\n",
    "    print(f\"Hyperparameter tuning complete. Used {parallel_jobs} parallel jobs... Took {complete_time:.2f} seconds.\\n\")\n",
    "\n",
    "def print_feature_importances(ML_name, mdl):\n",
    "    print(\"==\"*16,\"feature_importances\",\"==\"*16)\n",
    "    for name, importance in zip(X_train.columns, mdl.feature_importances_):\n",
    "        print(name, \"==\", importance)\n",
    "    importances = mdl.feature_importances_\n",
    "    indices = np.argsort(importances)\n",
    "    features = X_train.columns\n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "    plt.xlabel('Relative Importances')\n",
    "    plt.show();\n",
    "\n",
    "def confusion_matrix_plot(confusion_matrix, classifier):\n",
    "    plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.get_cmap('Blues'))\n",
    "    labels = ['Nondemented', 'Demented']\n",
    "    plt.title('Confusion Matrix for {}'.format(classifier))\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    s = [['TN', 'FP'], ['FN', 'TP']]\n",
    "    for i, j in itertools.product(range(2), range(2)):\n",
    "        plt.text(j, i, str(s[i][j]) + \" = \" + str(confusion_matrix[i][j]))\n",
    "    plt.show()\n",
    "\n",
    "def grid(classifier, parameters,jobs):\n",
    "    ML_name = classifier.__class__.__name__\n",
    "    start_time = time()\n",
    "    models = ['DecisionTreeClassifier', 'RandomForestClassifier', 'XGBClassifier', 'GradientBoostingClassifier', 'AdaBoostClassifier']\n",
    "    if ML_name in models:\n",
    "        method = \"Randomized Grid Search\"\n",
    "        grid = RandomizedSearchCV(estimator=classifier, param_distributions=parameters, cv=10, n_iter=100, scoring=\"roc_auc\", random_state=42,n_jobs=jobs,verbose=0)\n",
    "    else:\n",
    "        method = \"Grid Search\"\n",
    "        grid = GridSearchCV(estimator=classifier, param_grid=parameters, cv=10, scoring=\"roc_auc\",n_jobs=jobs,verbose=0)\n",
    "    grid.fit(X_train, y_train)\n",
    "    fitting_time = time() - start_time\n",
    "    best_params = grid.best_params_\n",
    "    mdl = grid.best_estimator_.fit(X_train, y_train)\n",
    "    pred = mdl.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, pred)\n",
    "    test_recall = recall_score(y_test, pred, pos_label=1)\n",
    "    fpr,tpr,thresholds = roc_curve(y_test, pred, pos_label=1)\n",
    "    test_auc = auc(fpr, tpr)\n",
    "    total_fpr_opt[ML_name] = fpr  #Dit key\n",
    "    total_tpr_opt[ML_name] = tpr #Dict key\n",
    "    conf_matrix = confusion_matrix(y_test, pred)\n",
    "    complete_time = time() - start_time\n",
    "    # Feature importances if applicable\n",
    "    feature_importances = None\n",
    "    if ML_name in models:\n",
    "        feature_importances = True\n",
    "\n",
    "    # Inserting data into finale DataFrame\n",
    "    finale.loc[index, 'ML Model'] = ML_name\n",
    "    finale.loc[index, 'Method'] = method\n",
    "    finale.loc[index, 'fitting time'] = fitting_time\n",
    "    finale.loc[index, 'optimal hyperparameters'] = str(best_params)\n",
    "    finale.loc[index, 'best training AUC score'] = grid.best_score_\n",
    "    finale.loc[index, 'best estimator'] = str(mdl)\n",
    "    # Inserting test recall score and test AUC score into finale DataFrame\n",
    "    finale.loc[index, 'test recall score'] = test_recall\n",
    "    finale.loc[index, 'test AUC score'] = test_auc\n",
    "    print_summary(ML_name, mdl, method, fitting_time, best_params, conf_matrix, complete_time,jobs,feature_importances=feature_importances)\n",
    "    # Robustness check for certain models\n",
    "    robust_models = ['SVC', 'RandomForestClassifier', 'XGBClassifier']\n",
    "    if ML_name in robust_models:\n",
    "        print(\"------------- Calculating {} classification error for Robustness checks  -------------\\n\".format(ML_name))\n",
    "        if ML_name in models:\n",
    "            grid = RandomizedSearchCV(estimator=classifier, param_distributions=parameters, cv=10, n_iter=100, scoring=\"accuracy\")\n",
    "            robustment_test.loc[index, 'ML Model'] = ML_name\n",
    "            grid.fit(X_train, y_train)\n",
    "        else:\n",
    "            grid = GridSearchCV(estimator=classifier, param_grid=parameters, cv=10, scoring=\"accuracy\")\n",
    "            robustment_test.loc[index, 'ML Model'] = ML_name\n",
    "            grid.fit(X_train, y_train)\n",
    "        training_error = 1 - (grid.best_score_)\n",
    "        robustment_test.loc[index, 'Training Classification Error'] = training_error\n",
    "        mdl = grid.best_estimator_.fit(X_train, y_train)\n",
    "        pred = mdl.predict(X_test)\n",
    "        test_error = 1 - (accuracy_score(y_test, pred))\n",
    "        robustment_test.loc[index, 'Test Classification_Error'] = test_error\n",
    "        print(\"Training classification error: \", training_error)\n",
    "        print(\"Testing classification error: \", test_error)\n",
    "        print(\"\\n Robustness checks complete!\")\n",
    "\n",
    "\n",
    "index = 0\n",
    "for classifier, parameter in zip(classifiers, parameters):\n",
    "    grid(classifier, parameter,parallel_jobs)\n",
    "    index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finale.head(10)\n",
    "finale[['ML Model','test AUC score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check classification scores for the 3 main benchmark models\n",
    "robustment_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the ROC curve for each classifier ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Setting the colour of the ROC curves for each model\n",
    "colors = { 'LogisticRegression':'red',\n",
    "            'SVC':'green',\n",
    "            'DecisionTreeClassifier':'blue',\n",
    "            'AdaBoostClassifier':'yellow',\n",
    "            'RandomForestClassifier':'cyan',\n",
    "            'XGBClassifier':'magenta', \n",
    "            'GradientBoostingClassifier': 'black'\n",
    "        }    \n",
    "\n",
    "#total_fpr.keys() returns the models name as the key\n",
    "plt.figure(figsize=(20,12))\n",
    "for i in total_fpr_opt.keys():\n",
    "    colors = {'LogisticRegression':'red', 'SVC':'green', 'DecisionTreeClassifier':'blue', 'AdaBoostClassifier':'yellow','RandomForestClassifier':'cyan', 'XGBClassifier':'magenta', 'GradientBoostingClassifier': 'black', 'GaussianNB': 'white'}    \n",
    "    plt.plot(total_fpr_opt[i],total_tpr_opt[i],colors[i], lw=1, label=i)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.title('Comparison of ROC curves for all classifiers')\n",
    "plt.plot([0, 1], [0, 1], color='darkorange', lw=2, linestyle='--')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Hyperparameter refinement using hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Performance(model, y, X, fitting_time, best_params):\n",
    "    predictions = model.predict(X)\n",
    "    recall = recall_score(y, predictions, pos_label=1)\n",
    "    fpr, tpr, _ = roc_curve(y, predictions)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    print('Recall score: {:.4f}'.format(recall))\n",
    "    print('AUC score: {:.4f}'.format(auc_score))\n",
    "    \n",
    "    # Update finale table\n",
    "    global index\n",
    "    finale.loc[index, 'ML Model'] = model.__class__.__name__\n",
    "    finale.loc[index, 'Method'] = 'Hyperopt tuning'\n",
    "    finale.loc[index, 'fitting time'] = str(fitting_time)\n",
    "    finale.loc[index, 'optimal hyperparameters'] = str(best_params)\n",
    "    finale.loc[index, 'best training AUC score'] = auc_score\n",
    "    finale.loc[index, 'best estimator'] = str(model)\n",
    "    finale.loc[index, 'test recall score'] = recall\n",
    "    finale.loc[index, 'test AUC score'] = auc_score\n",
    "    \n",
    "    # Increment index for the next entry\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<font color=purple>*XGBoost hyperopt*</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params = {\n",
    "        'n_estimators': params['n_estimators'],\n",
    "        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'gamma': \"{:.3f}\".format(params['gamma']),\n",
    "        'min_child_weight': \"{:.3f}\".format(params['min_child_weight']),\n",
    "        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "        'random_state': params['random_state']\n",
    "    }\n",
    "    \n",
    "    #clf = XGBClassifier(n_jobs=4,**params)\n",
    "    clf = XGBClassifier(**params)\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True,random_state=42)\n",
    "    score = cross_val_score(clf, X_train, y_train, scoring='roc_auc', cv=StratifiedKFold()).mean()\n",
    "    print(\"roc_auc {:.3f} params {}\".format(score, params))\n",
    "    training_scores.append(score)\n",
    "    return score\n",
    "\n",
    "space = {\n",
    "    'n_estimators': scope.int(hp.uniform('n_estimators',100, 2000)),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.4),\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 2, 14, 1)),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0),\n",
    "    'min_child_weight': hp.uniform('min_child_weight',0.1, 0.6),\n",
    "    'gamma': hp.uniform('gamma', 0.0, 0.5),\n",
    "    'random_state': 42 \n",
    "}\n",
    "\n",
    "#Capture training scores & fitting times as well\n",
    "training_scores = []\n",
    "start_time = time()\n",
    "best = fmin(fn=objective,space=space,algo=tpe.suggest,max_evals=10)\n",
    "best_params = space_eval(space, best)\n",
    "fitting_time = time() - start_time\n",
    "print(\"\\nHyperopt completed in {} seconds\\n\".format(fitting_time))\n",
    "print(\"The average training score was {} \". format(sum(training_scores)/len(training_scores)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the best parameters computed using Hypteropt to calculate a **Recall** and **AUC** score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Call Performance function after hyperparameter tuning\n",
    "mdl = XGBClassifier(**best_params)\n",
    "mdl.fit(X_train, y_train)\n",
    "Performance(mdl, y_test, X_test, fitting_time, best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<font color=Purple>*RandomForest hyperopt*</color>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params = {\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'n_estimators':int(params['n_estimators']),\n",
    "        'random_state': params['random_state']\n",
    "    }\n",
    "    \n",
    "    #clf = RandomForestClassifier(n_jobs=4,**params)\n",
    "    clf = RandomForestClassifier(**params)\n",
    "    score = cross_val_score(clf, X_train, y_train, scoring='roc_auc', cv=StratifiedKFold()).mean()\n",
    "    print(\"roc_auc {:.3f} params {}\".format(score, params))\n",
    "    training_scores.append(score)\n",
    "    return score\n",
    "\n",
    "space = {\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 10,30, 1)),\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 100,1500,25)),\n",
    "    'random_state': 42 }\n",
    "\n",
    "#Capture training scores & fitting times as well\n",
    "training_scores = []\n",
    "start_time = time()\n",
    "best = fmin(fn=objective,space=space,algo=tpe.suggest,max_evals=10)\n",
    "best_params = space_eval(space, best)\n",
    "fitting_time = time() - start_time\n",
    "print(\"\\nHyperopt completed in {} seconds\\n\".format(fitting_time))\n",
    "print(\"The average training score was {} \". format(sum(training_scores)/len(training_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Call Performance function after hyperparameter tuning\n",
    "mdl = RandomForestClassifier(**best_params)\n",
    "mdl.fit(X_train, y_train)\n",
    "Performance(mdl, y_test, X_test, fitting_time, best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<font color=Purple>*SVC hyperopt*</color>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params = {\n",
    "        'C': params['C'],\n",
    "        'gamma':params['gamma'],\n",
    "        'kernel': params['kernel'],\n",
    "        'random_state': params['random_state']\n",
    "    }\n",
    "    \n",
    "    clf = SVC(**params)\n",
    "    score = cross_val_score(clf, X_train, y_train, scoring='roc_auc', cv=StratifiedKFold()).mean()\n",
    "    print(\"roc_auc {:.3f} params {}\".format(score, params))\n",
    "    training_scores.append(score)\n",
    "    return score\n",
    "\n",
    "space = {\n",
    "    'C': hp.uniform('C', 10,11),\n",
    "    'gamma': hp.quniform('gamma', 0.35,0.42,0.01),    \n",
    "    'kernel': 'rbf', \n",
    "    'random_state': 42 }\n",
    "\n",
    "#Capture training scores & fitting times as well\n",
    "training_scores = []\n",
    "start_time = time()\n",
    "best = fmin(fn=objective,space=space,algo=tpe.suggest,max_evals=10)\n",
    "best_params = space_eval(space, best)\n",
    "fitting_time = time() - start_time\n",
    "print(\"\\nHyperopt completed in {} seconds\\n\".format(fitting_time))\n",
    "print(\"The average training score was {} \". format(sum(training_scores)/len(training_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Call Performance function after hyperparameter tuning\n",
    "mdl = SVC(**best_params)\n",
    "mdl.fit(X_train, y_train)\n",
    "Performance(mdl, y_test, X_test, fitting_time, best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Results\n",
    "---\n",
    "\n",
    "##### 4.1 Model Evaluation and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to compare the 2 dataframes -- finale & benchmark -- to show the differences due to parameter tuning ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(finale, benchmark):\n",
    "    # Rename column in benchmark DataFrame\n",
    "    benchmark.rename(columns={'ML Algo name': 'ML Model'}, inplace=True)\n",
    "    \n",
    "    # Extract relevant columns from finale DataFrame and sort by ML Model\n",
    "    output = finale[['ML Model', 'Method', 'test AUC score']].sort_values('ML Model')\n",
    "    \n",
    "    # Set Method column to 'untuned' in benchmark DataFrame\n",
    "    benchmark['Method'] = 'untuned'\n",
    "    \n",
    "    # Extract relevant columns from benchmark DataFrame\n",
    "    benchmark = benchmark[['ML Model', 'Method', 'test AUC score']]\n",
    "    \n",
    "    # Concatenate output and benchmark DataFrames, sort by ML Model, and set index\n",
    "    df = pd.concat([output, benchmark]).sort_values('ML Model')\n",
    "    df.set_index('ML Model', inplace=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = process_results(finale,benchmark)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'untuned' test AUC score for each ML Model and store it in a dictionary\n",
    "untuned_dict = {}\n",
    "for model, group in output[output['Method'] == 'untuned'].groupby('ML Model'):\n",
    "    untuned_dict[model] = group.iloc[0]['test AUC score']\n",
    "\n",
    "# List of methods to check against\n",
    "methods_to_check = ['Randomized Grid Search', 'Hyperopt tuning', 'Grid Search']\n",
    "\n",
    "# Print out the test AUC scores for each ML model with Randomized Grid Search method that outperformed the untuned method\n",
    "for model, group in output.groupby('ML Model'):\n",
    "    if any(method in group['Method'].values for method in methods_to_check):\n",
    "        for method in methods_to_check:\n",
    "            if method in group['Method'].values:\n",
    "                method_score = group[group['Method'] == method]['test AUC score'].values[0]\n",
    "                if model in untuned_dict and method_score > untuned_dict[model]:\n",
    "                    print(f\"The following model and optimisation method outperformed its benchmark value of {untuned_dict[model]}\")\n",
    "                    print(f\"ML Model: {model}\")\n",
    "                    print(f\"Method: {method}\")\n",
    "                    print(f\"Test AUC score: {method_score}\")\n",
    "                    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5: Conclusion\n",
    "---\n",
    "\n",
    "##### 5.1 Free-form visualization of SVM Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define a meshgrid function that will plot the decision surface\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "#Select 2 features randomly to compare - SES / EDUC in this case\n",
    "#Convert to array\n",
    "X = np.array(X_test)\n",
    "X = X[:, [2,3]]\n",
    "\n",
    "#ax1 --> benchmark model\n",
    "#ax2 --> optimised model\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, sharey=True, figsize=(20,10))\n",
    "title = ('Decision surface for Benchmark model')\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "mdl = SVC(C=1, gamma='auto', kernel='rbf', random_state=42).fit(X, y_test)\n",
    "plot_contours(ax1, mdl, xx, yy, cmap=plt.cm.coolwarm, alpha=0.7)\n",
    "ax1.scatter(X0, X1, c=y_test, cmap=plt.cm.coolwarm, s=20, marker='x', edgecolors='k')\n",
    "ax1.set_xlabel(X_test.columns[2])\n",
    "ax1.set_ylabel(X_test.columns[3])\n",
    "ax1.set_xticks(())\n",
    "ax1.set_yticks(())\n",
    "ax1.legend()\n",
    "ax1.set_title(title);\n",
    "\n",
    "\n",
    "title = ('Decision surface for Optimized model')\n",
    "mdl = SVC(C=10.94, gamma=0.36, kernel='rbf', random_state=42).fit(X, y_test)\n",
    "plot_contours(ax2, mdl, xx, yy, cmap=plt.cm.coolwarm, alpha=0.7)\n",
    "ax2.scatter(X0, X1, c=y_test, cmap=plt.cm.coolwarm, s=25, marker='x', edgecolors='k')\n",
    "ax2.set_xlabel(X_test.columns[2])\n",
    "ax2.set_ylabel(X_test.columns[3])\n",
    "ax2.set_xticks(())\n",
    "ax2.set_yticks(())\n",
    "ax2.set_title(title);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_project_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
